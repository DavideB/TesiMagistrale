% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode=true}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
]{article}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provides euro and other symbols
\else % if luatex or xelatex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
  hidelinks,
}
\urlstyle{same} % disable monospaced font for URLs
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
% Redefines (sub)paragraphs to behave more like sections
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother


\date{}

\begin{document}

\hypertarget{xspark-dagsymb}{%
\section{xSpark-dagsymb}\label{xspark-dagsymb}}

A tool exploiting symbolic execution techniques to safely run multi-dag
applications in xSpark. (https://github.com/gioenn/xSpark-dagsymb.git).
It combines two distinct functionalities, application profiling and
application execution, which are part of symbolic executor enabled
xSpark applications lifecycle, in one integrated tool called
\textbf{xSpark-dagsymb}.

The tool is composed by ten principal modules:
\textbf{xSpark\_dagsymb.py}, \textbf{launch.py}, \textbf{run.py},
\textbf{log.py}, \textbf{plot.py}, \textbf{metrics.py},
\textbf{configure.py}, \textbf{processing.py},
\textbf{average\_runs.py}, \textbf{process\_on\_server.py}, in addition
to the configuration files \textbf{credentials.json},
\textbf{setup.json}, \textbf{control.json}.

\hypertarget{core-functionality}{%
\subsubsection{Core Functionality}\label{core-functionality}}

The \textbf{launch.py} module manages the startup of spot request
instances on \emph{Amazon EC2} or virtual machines on \emph{Microsoft
Azure} and waits until the instances are created and are reachable from
the network via their public ip's. Subsequently the \textbf{run.py}
module receives as input the instances on which to configure the cluster
(\emph{HDFS} or \emph{Spark}), configures and runs the applications to
be executed and waits for the applications to complete. The module
\textbf{log.py} downloads and saves the logs created by the applications
run. The \textbf{plot.py} and \textbf{metrics.py} modules respectively
generate graphs and calculate metrics. The
\textbf{process\_on\_server.py} module can be called to remotely execute
the log analysis, graphs generation and metrics calculation on the
xSpark master server, and download the results to the client. This
option is very useful to speed-up the processing especially in case of
sizeable logfiles.

\hypertarget{cloud-environment-configuration}{%
\subsubsection{Cloud Environment
Configuration}\label{cloud-environment-configuration}}

The Cloud environment must be properly initialized in order to allow
\textbf{xSpark\_dagsymb} to access and modify resources in the cloud.

\hypertarget{azure}{%
\subparagraph{Azure}\label{azure}}

Follow the instructions to create an identity called
\href{https://docs.microsoft.com/en-us/azure/azure-resource-manager/resource-group-create-service-principal-portal}{service
principal} and assign to it all the required permissions:

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\item
  Check that your account has the
  \href{https://docs.microsoft.com/en-us/azure/azure-resource-manager/resource-group-create-service-principal-portal?view=azure-cli-latest\#required-permissions}{required
  permissions} to create an identity.
\item
  Create an
  \href{https://docs.microsoft.com/en-us/azure/azure-resource-manager/resource-group-create-service-principal-portal?view=azure-cli-latest\#create-an-azure-active-directory-application}{Azure
  Active Directory application}
\item
  Get the
  \href{https://docs.microsoft.com/en-us/azure/azure-resource-manager/resource-group-create-service-principal-portal?view=azure-cli-latest\#get-application-id-and-authentication-key}{\emph{Application
  ID} and an \emph{Authentication Key}}. The \emph{Application ID} and
  \emph{Authentication Key} values replace respectively the
  \emph{\textless{} AZ-APP-ID \textgreater{}} and the \emph{\textless{}
  AZ-SECRET \textgreater{}} values in the credentials.json file
  described in the next paragraph.
\end{enumerate}

\hypertarget{tool-configuration}{%
\subsubsection{Tool Configuration}\label{tool-configuration}}

The \textbf{configure.py} module contains the Config class used to
instantiate configuration objects that are initialized with default
values. The \textbf{credentials.json} file contains \emph{Amazon EC2}
and/or \emph{Microsoft Azure} credential information. The
\textbf{setup.json} contains Cloud environment and \emph{Amazon EC2}
and/or \emph{Microsoft Azure} image parameters. The
\textbf{control.json} file contains xSpark controller configuration
parameters. Information in the \textbf{credentials.json},
\textbf{setup.json} and \textbf{control.json} files are used to
customize the configuration object used by other modules during the
application execution.

\begin{itemize}
\item
  AWS and/or MS-Azure Credentials: Open the
  \emph{credentials\_template.json} file and add the credentials for
  \textbf{xSpark\_dagsymb} (see instructions below to retrieve missing
  credentials):

  \{ ``AzTenantId'': ``\textless{} AZ-TENANT-ID \textgreater{}'',
  ``AzSubscriptionId'': ``\textless{} AZ-SUBSCRIPTION-ID
  \textgreater{}'', ``AzApplicationId'': ``\textless{} AZ-APP-ID
  \textgreater{}'', ``AzSecret'': ``\textless{} AZ-SECRET
  \textgreater{}'', ``AzPubKeyPath'': ``\textless{} AZ-PUB-KEY-PATH
  \textgreater{}'', ``AzPrvKeyPath'': ``\textless{} AZ-PRV-KEY-PATH
  \textgreater{}'', ``AwsAccessId'': ``\textless{} KEY-ID
  \textgreater{}'', ``AwsSecretId'': ``\textless{} ACCESS-KEY
  \textgreater{}'', ``KeyPairPath'': ``\textless{} KEY-PAIR-PATH
  \textgreater{}'' \}
\end{itemize}

Save the file as \emph{credentials.json}.

\begin{itemize}
\tightlist
\item
  How to retrieve your Azure credentials (using the Azure Command Line
  Interface):
\end{itemize}

Install the
\href{https://docs.microsoft.com/it-it/cli/azure/install-azure-cli?view=azure-cli-latest}{Azure
CLI}. Launch the following command from a console terminal:

\begin{verbatim}
$ az login
Note, we have launched a browser for you to login. For old experience with device code, use "az login --use-device-code"
\end{verbatim}

a browser authentication windows is open to allow you to login to the
Azure portal. If login is successful, you should get an output similar
to the following:

\begin{verbatim}
You have logged in. Now let us find all the subscriptions to which you have access...
[
  {
    "cloudName": "AzureCloud",
    "id": "< AZ-SUBSCRIPTION-ID >",
    "isDefault": true,
    "name": "Microsoft Azure Sponsorship xx",
    "state": "Enabled",
    "tenantId": "< AZ-TENANT-ID >",
    "user": {
      "name": "*your_username*",
      "type": "user"
    }
  }
]
\end{verbatim}

where you can pick the \emph{\textless{} AZ-SUBSCRIPTION-ID
\textgreater{}} and \emph{\textless{} AZ-TENANT-ID \textgreater{}}
parameters to be written in the \emph{credentials.json} file.

Launch the following command from a console terminal to create the
private and public RSA cryptography keys:

\begin{verbatim}
$ ssh-keygen -t rsa
\end{verbatim}

Save the generated files in your favorite folder and replace the values
\emph{\textless{} AZ-PUB-KEY-PATH \textgreater{}} and \emph{\textless{}
AZ-PRV-KEY-PATH \textgreater{}} in the \emph{credentials.json} file
respectively with the fully qualified file name of the public and the
private key.

\begin{itemize}
\item
  Setup the xSpark and the Virtual Machine Cloud environment: Edit the
  setup.json file to set the values to your need. The following is an
  example using Microsoft Azure VM Cloud Service:

  \{ ``Provider'': ``AZURE'', ``VM'': \{ ``Core'': 16, ``Memory'':
  ``100g'' \}, ``ProcessOnServer'': true, ``InstallPython3'': false,
  ``Aws'': \{ ``SecurityGroup'': ``spark-cluster'', ``Region'':
  ``us-west-2'', ``EbsOptimized'': true, ``Price'': ``0.015'',
  ``InstanceType'': ``m3.medium'', ``AwsRegions'': \{ ``eu-west-1'':
  \{``ami'':``ami-bf61fbc8'', ``az'': ``eu-west-1c'', ``keypair'':
  ``simone'', ``price'': ``0.0035'' \}, ``us-west-2'': \{``ami'':
  ``ami-7f5ff81f'', ``snapid'': ``snap-4f38bf1c'', ``az'':
  ``us-west-2c'', ``keypair'': ``simone2'', ``price'': ``0.015''\} \}
  \}, ``Azure'': \{ ``ResourceGroup'': ``xspark-davide-ap'',
  ``SecurityGroup'': ``cspark-securitygroup2'', ``StorageAccount'': \{
  ``Sku'': ``standard\_lrs'', ``Kind'': ``storage'', ``Name'':
  ``xsparkstoragedavide1'' \}, ``Subnet'': ``default'', ``NodeSize'':
  ``Standard\_D14\_v2\_Promo'', ``Network'': ``cspark-vnet2'',
  ``Location'': ``australiaeast'', ``NodeImage'': \{ ``BlobContainer'':
  ``vhd'', ``StorageAccount'': ``xsparkstoragedavide1'', ``Name'':
  ``vm2-os.vhd'' \} \}, ``Spark'': \{ ``ExternalShuffle'': ``true'',
  ``Home'': ``/opt/spark/'', ``LocalityWaitRack'': 0, ``CpuTask'': 1,
  ``LocalityWaitProcess'': 1, ``LocalityWait'': 0, ``LocalityWaitNode'':
  0 \}, ``xSpark'': \{ ``Home'': ``/usr/local/spark/'' \}, ``SparkSeq'':
  \{ ``Home'': ``/opt/spark-seq/'' \}
\item
  Setup the Spark Controller parameters: Edit the control.json file to
  set the values to your need. The following is an example:

  \{\\
  ``Alpha'': 1.0, ``Beta'': 0.3, ``OverScale'': 2, ``K'': 50, ``Ti'':
  12000, ``TSample'': 500, ``Heuristic'': ``CONTROL\_UNLIMITED'',
  ``CoreQuantum'': 0.05, ``CoreMin'': 0, ``CpuPeriod'': 100000 \}
\end{itemize}

\hypertarget{application-profiling}{%
\subsubsection{Application Profiling}\label{application-profiling}}

Profiling is the first logical phase of the performance testing
lifecycle. In profiling mode, Benchmarks are run using the ``vanilla''
Spark version. Then the \textbf{processing.py} module is called to
analyze the logs and create the ``application profile'', that is a JSON
file containing the annotated DAG of the executed stages plus additional
information intended to be used by the controller in the execution
phase. The \textbf{average\_runs.py} module is called to create a JSON
profile called *dagsymbmarkname\textgreater-app.json containing the
average values of the ``n'' profiles obtained by running the same
application ``n'' times. Finally, the file with the average profile is
uploaded to the xSpark configuration directory on the master server.

\hypertarget{application-execution}{%
\subsubsection{Application Execution}\label{application-execution}}

Benchmarks are executed using
\href{https://github.com/gioenn/xSpark.git}{xSpark}, and require the
application profile \emph{dagsymbmarkname}-app.json to be present in the
xSpark configuration directory. The name of the application and the
parameters to modify its default configuration can either be specified
as commandline arguments to the \emph{submit} command, or can be
inserted into JSON format ``experiment files'' and passed as commandline
arguments to the \emph{launch\_exp} command. As an example, an
experiment files for Pagerank , one for KMeans and one for
AggregateByKey are shown here below:

PageRank experiment file example:

\begin{verbatim}
{
    "Deadline": 148080,
    "BenchmarkName": "PageRank",
    "BenchmarkConf": {
            "NumOfPartitions": 1000,
            "NumV": 35000000,
            "Mu": 3.0,
            "Sigma": 0.0,
            "MaxIterations": 1,
            "NumTrials": 1     
        }
}
\end{verbatim}

KMeans experiment file example:

\begin{verbatim}
{
    "Deadline": 116369,
    "BenchmarkName": "KMeans",
    "BenchmarkConf": {
            "NumOfPartitions": 1000,
            "NumOfPoints": 100000000,
            "NumOfClusters": 10,
            "Dimensions": 20,
            "Scaling": 0.6,
            "MaxIterations": 1
        }
 }
\end{verbatim}

AggregateByKey experiment file example:

\begin{verbatim}
{
    "Deadline": 124000,
    "BenchmarkName": "scala-agg-by-key",
    "BenchmarkConf": {
            "ScaleFactor": 5
            }
 }
\end{verbatim}

\hypertarget{download-requirements}{%
\subsubsection{Download \& Requirements}\label{download-requirements}}

\begin{verbatim}
$ git clone https://github.com/gioenn/xSpark-dagsymb.git
$ cd xSpark-dagsymb
$ pip3 install -r requirements.txt"
\end{verbatim}

\hypertarget{xspark-dagsymb-commands}{%
\subsection{xSpark-dagsymb commands}\label{xspark-dagsymb-commands}}

\href{https://github.com/gioenn/xSpark-dagsymb}{xSpark-dagsymb} run
command syntax:

\begin{verbatim}
$ cd xSpark-dagsymb
$ python3 xSpark_dagsymb.py *command [*args*]*
\end{verbatim}

\hypertarget{command-args-syntax}{%
\subparagraph{*command {[}*args*{]}*
syntax:}\label{command-args-syntax}}

\begin{verbatim}
[setup | reboot | terminate | log | profiling | time_analysis | check | 
 profile | submit | launch_exp] [*args*]
\end{verbatim}

where *args* is a set of command-specific arguments list or options.

\hypertarget{setup-command-syntax}{%
\subparagraph{\texorpdfstring{\emph{setup} command
syntax:}{setup command syntax:}}\label{setup-command-syntax}}

\begin{verbatim}
setup [hdfs | spark | all | generic] {[-n | --num-instances] *numinstances*} 
        {[-y |  --assume-yes]}
\end{verbatim}

where *numinstances* is the number of nodes to add to the specified
cluster (default is 5), \emph{-y} or \emph{--assume-yes} option sets
default affirmative answer to interactive confirmation requests.

\hypertarget{reboot-command-syntax}{%
\subparagraph{\texorpdfstring{\emph{reboot} command
syntax:}{reboot command syntax:}}\label{reboot-command-syntax}}

\begin{verbatim}
reboot [hdfs | spark | all | generic]
\end{verbatim}

reboots all nodes in the specified cluster.

\hypertarget{terminate-command-syntax}{%
\subparagraph{\texorpdfstring{\emph{terminate} command
syntax:}{terminate command syntax:}}\label{terminate-command-syntax}}

\begin{verbatim}
terminate [hdfs | spark | all | generic]
\end{verbatim}

deletes (destroyes) all nodes and their connected resources in the
specified cluster.

\hypertarget{check-command-syntax}{%
\subparagraph{\texorpdfstring{\emph{check} command
syntax:}{check command syntax:}}\label{check-command-syntax}}

\begin{verbatim}
check [hdfs | spark | all | generic]
\end{verbatim}

checks the status of all nodes in the specified cluster.

\hypertarget{profile-command-syntax}{%
\subparagraph{\texorpdfstring{\emph{profile} command
syntax:}{profile command syntax:}}\label{profile-command-syntax}}

\begin{verbatim}
profile [*exp_file_paths*] {[-r | --num-runs] *numruns*} {[-R | --reuse-dataset]} 
         {[-q | --spark-seq]}      
\end{verbatim}

where *exp\_file\_paths* is a non-empty space separated list of
experiment file paths, *numruns* is the number of times to repeat the
profiling for each experiment file (default is 1), \emph{-R} or
\emph{--reuse-dataset} option instructs xSpark to reuse (not to delete)
application data in hdfs master node, \emph{-q} or \emph{--spark-seq}
option instructs xSpark to use Spark data sequencing home directory.

\hypertarget{submit-command-syntax}{%
\subparagraph{\texorpdfstring{\emph{submit} command
syntax:}{submit command syntax:}}\label{submit-command-syntax}}

\begin{verbatim}
submit [*exp_file_paths*] {[-r | --num-runs] *numruns*} {[-R | --reuse-dataset]}       
\end{verbatim}

where *exp\_file\_paths* is a non-empty space separated list of
experiment file paths, *numruns* is an integer specifying the number of
times to repeat the profiling for each experiment file (default is 1),
\emph{-R} or \emph{--reuse-dataset} option instructs xSpark to reuse
(not to delete) application data in hdfs master node.

\hypertarget{launch_exp-command-syntax}{%
\subparagraph{\texorpdfstring{\emph{launch\_exp} command
syntax:}{launch\_exp command syntax:}}\label{launch_exp-command-syntax}}

\begin{verbatim}
launch_exp {[-e | --executors] *numexecutors*} {[-b | --application] [pagerank | kmeans | 
             sort_by_key]} {[-v | --variable-parameter] *bpar*} 
             {[-n | --num-instances] *numinstances*} 
\end{verbatim}

where *numexecutors* is an integer specifying the maximum number of
executors to be used in the experiments, *bpar* is a variable parameter
representing num\_v for pagerank, num\_of\_points for kmeans,
scale\_factor for sort\_by\_key, \emph{-r} or \emph{--num-runs} is the
number of times the specified application is run, \emph{-p} or
\emph{--num-partitions} is the number of partitions for each task,
\emph{-P} or \emph{--profile} instructs xSpark to perform the profiling
at the end of the experiments, \emph{-R} or \emph{--reuse-dataset}
option instructs xSpark to reuse (do not delete) application data in
hdfs master node.

\hypertarget{log_profiling-command-syntax}{%
\subparagraph{\texorpdfstring{\emph{log\_profiling} command
syntax:}{log\_profiling command syntax:}}\label{log_profiling-command-syntax}}

\begin{verbatim}
log_profiling {[-L | --local]}
\end{verbatim}

where \emph{-L} or \emph{--local} option instructs xSpark use default
local output folders.

\hypertarget{time_analysis-command-syntax}{%
\subparagraph{\texorpdfstring{\emph{time\_analysis} command
syntax:}{time\_analysis command syntax:}}\label{time_analysis-command-syntax}}

\begin{verbatim}
time_analysis {[-i | --input-dir] *dir*}
\end{verbatim}

where \emph{dir} is the directory where the log files are located.

\hypertarget{example-profile-and-test-pagerank}{%
\subsection{Example: Profile and Test
PageRank}\label{example-profile-and-test-pagerank}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\item
  Create the credential.json file as instructed above.
\item
  Configure the setup.json file as instructed above.
\item
  Configure the control.json file as instructed above.
\item
  Create and initialize a hdfs cluster with 5 nodes:

  \$ python3 xSpark\_dagsymb.py setup hdfs -n 5
\item
  Create and initialize a spark cluster with 5 nodes:

  \$ python3 xSpark\_dagsymb.py setup spark -n 5
\item
  Create \emph{experiment.json} file with the the following contents:

  \{ ``Deadline'': 148080, ``BenchmarkName'': ``PageRank'',
  ``BenchmarkConf'': \{ ``NumOfPartitions'': 1000, ``NumV'': 35000000,
  ``Mu'': 3.0, ``Sigma'': 0.0, ``MaxIterations'': 1, ``NumTrials'': 1\\
  \} \}
\item
  Run the Profiling with 6 iterations:

  \$ python3 xSpark\_dagsymb.py profile experiment.json -r 6 -R
\item
  Run the Application Test with 5 iterations:

  \$ python3 xSpark\_dagsymb.py submit experiment.json -r 5 -R
\end{enumerate}

\hypertarget{todo}{%
\subsubsection{TODO}\label{todo}}

\begin{itemize}
\tightlist
\item[$\square$]
  complete this file with installation instructions for AWS
\item[$\square$]
  clean-up code
\end{itemize}

\end{document}
