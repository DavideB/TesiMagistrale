%*******************************************************
% Abstract
%*******************************************************
%\renewcommand{\abstractname}{Abstract}
\addcontentsline{toc}{chapter}{\abstractname}

\pdfbookmark[1]{Abstract}{Abstract}
\begingroup
\let\clearpage\relax
\let\cleardoublepage\relax
\let\cleardoublepage\relax

\chapter*{Abstract}
\lettrine[lines=4]{\textcolor{purple}{T}}{he} need to crunch a steadily growing amount of data generated by the modern applications is driving an increasing demand of flexible computing power, that is more and more often satisfied by cloud computing solutions. Cloud computing has revolutionized the way computer infrastructures are abstracted and used. It is built on virtual hardware and software infrastructures accessible via the Internet and its usage is suitable for big data processing by enterprises of any size. 
Big data is a research field that deals with ways to analyze and extract information from data sets containing structured and unstructured data whose size is so large that makes the processing of data with traditional databases and applications very difficult or pratically impossible. The processing of these data therefore requires the use of distributed frameworks specialized for the parallel execution of programs, such as Apache Hadoop~\cite{misc:ApacheHadoop} and Apache Spark~\cite{misc:ApacheSpark}. These specialized frameworks are used to transform the applications in atomic parts that can be executed in a distributed cluster of physical or virtual machines. The limit to the level of parallelism that can be obtained is given by the number of machines and the amount of synchronization needed between the data fragments representing the intermediate results. This paradigm has been historically represented by the Map-Reduce programming model firstly introduced by Google~\cite{misc:GoogleMapReduce} and subsequently implemented by the Apache Hadoop~\cite{misc:ApacheHadoop} framework. Map-Reduce consists of two distinct tasks: Map and Reduce. As suggested by the name, the reducer phase takes place after the mapper phase has been completed. A map job reads and processes a block of data to produce key-value pairs as intermediate outputs, that are input to the reducer. The reducer receives the key-value pair from multiple map jobs and then aggregates those intermediate data tuples (intermediate key-value pair) into a smaller set of tuples or key-value pairs which is the final output.
A more advanced solutions is represented by Apache Spark~\cite{misc:ApacheSpark}, that provide a greater flexibility and allow building large-scale data processing applications using a DAG based structure. It generalizes the two stage Map-Reduce to support arbitrary DAG. The main advantage of Spark with respect to previous cluster computing frameworks is the fast data sharing between operations. For example, Apache Hadoop requires intermediate data to be written to disk in order to be accessible by the following operations, Spark instead allows to execute in-memory computing. Moreover, big data applications pose new challenges in satisfying requirements on the Quality of Service (QoS) provided to end users. 
In the world of traditional applications (e.g., web) this problem has often been faced using self-adaptive systems that control runtime \textit{KPIs} (Key Performance Indicators) (e.g.  response time) against changes in the application context and workload. In the world of big data, the notion of \qos differ by application type. While interactive applications are usually assessed according to response time or throughput, and their fulfillment depends on the intensity and variety of the incoming requests,  big data applications might require a single batch computation on a very large dataset, thus \qos must consider the execution of a single run. In this domain \qos is often called deadline, or the desired duration of the computation. Thus, users may be interested in quantifying and constraining the execution time of every single run of an application. Cluster computing, an implementation of the Parallel Computing paradigm composed by a large number of networked multi-cpu computers in the cloud, is widely used to speed-up application execution time. 

However, speed alone is not enough to guarantee that a big data application will meet a fixed deadline. The speed of execution of a big data application in fact depends on many factors that vary over time: they can be endogenous factors, such as the amount of resources available at a certain moment (ie number of cpu cores, amount of memory, disk space) or exogenous factors, such as the amount of data to be processed.

One of the most frequently used cluster computing frameworks for big data analytics is Apache Spark, which provides a fast and general, fault-tolerant data processing platform that allows quick in memory computation. Spark computation is based on RDDs, a data abstraction, and DAGs, representing the data manipulation processes. xSpark, developed at Politecnico di Milano, is an extension of the Apache Spark framework that offers fine-grained dynamic resource allocation using lightweight containers. It allows users to constrain the duration of the execution of an application by specifying a deadline. This is possible thanks to the knowledge of the application Directed Acyclic Graph (DAG), generated by running the application in profiling mode, and the runtime allocation of resources to task executors by a specialized xSpark’s control loop, composed by a centralized heuristic and a distributed local controller. All the above works under the assumption that the application execution flow is represented by a single DAG, which is true when the application code does not contain any conditional branch whose outcome depends on user input values or the result of previous calculations involving input data. When the application contains such conditional branches, a family of DAGs (or a tree of DAGs) is needed to describe all the possible execution flows corresponding to the combinations of all the different branch outcomes. Here is where xSpark-dagsymb, the project described in this thesis, comes into play. xSpark-dagsymb extends xSpark capability to safely run multi-DAG applications, by exploiting symbolic execution (techniques, principles or theory?). At each decisional branch outcome in the application, xSpark-dagsymb determines which DAGs are still valid and prunes the DAG tree, removing the invalid DAGs, thus leaving only the valid ones in the DAG tree. A heuristic is used to select the DAG to execute among the valid ones, in order to minimize the risk of missing the deadline while maximizing the CPU usage efficiency.
\vfill
\newpage
\pdfbookmark[1]{Sommario}{Sommario}
\chapter*{Sommario}
Per abstract si intende il sommario di un documento, senza l'aggiunta di interpretazioni e valutazioni. L'abstract si limita a riassumere, in un determinato numero di parole, gli aspetti fondamentali del documento esaminato. Solitamente ha forma "indicativo-schematica"; presenta cioé notizie sulla struttura del testo e sul percorso elaborativo dell'autore.

Max 2200 caratteri compresi gli spazi.

\endgroup