%*******************************************************
% Abstract
%*******************************************************
%\renewcommand{\abstractname}{Abstract}
\addcontentsline{toc}{chapter}{\abstractname}

\pdfbookmark[1]{Abstract}{Abstract}
\begingroup
\let\clearpage\relax
\let\cleardoublepage\relax
\let\cleardoublepage\relax

\chapter*{Abstract}
\lettrine[lines=4]{\textcolor{purple}{T}}{he} need to crunch a steadily growing amount of data generated by the modern applications is driving an increasing demand of flexible computing power,  often satisfied by cloud computing solutions. Cloud computing  revolutionized the way computer infrastructures are abstracted and used by exploiting virtualization and providing an easy access to resources through the Internet~\cite{articleBigData:2017}.
%It is built on virtual hardware and software infrastructures accessible via the Internet and its usage is suitable for big data processing by enterprises of any size. 


Big data is a research field that deals with ways to analyze and extract information from data sets containing structured and unstructured data whose size is so large that makes the processing of data with traditional databases and applications very difficult or practically impossible. The processing of these data therefore requires the use of distributed frameworks specialized for the parallel execution of programs, such as Apache Hadoop~\cite{misc:ApacheHadoop} and Apache Spark~\cite{misc:ApacheSpark}. These specialized frameworks are used to transform the applications in atomic parts that can be executed in a distributed cluster of physical or virtual machines. 

%The limit to the level of parallelism that can be obtained is given by the number of machines and the amount of synchronization needed between the data fragments representing the intermediate results. 
Hadoop uses the Map-Reduce programming model that was firstly introduced by Google~\cite{misc:GoogleMapReduce} and consists of two distinct phases: Map and Reduce. %As suggested by the name, the reducer phase takes place after the mapper phase has been completed. 
%A map job reads and processes a block of data to produce key-value pairs as intermediate outputs, that are input to the reducer. The reducer receives the key-value pair from multiple map jobs and then aggregates those intermediate data tuples (intermediate key-value pair) into a smaller set of tuples or key-value pairs which is the final output.
The former processes and \textit{transforms} a block of data to produce key-value pairs (tuples) as intermediate outputs, the latter \textit{aggregates} these tuples into a final output.
%One of the most frequently used cluster computing frameworks for big data analytics is Apache Spark (Spark), which provides a fast and general, fault-tolerant data processing platform that allows quick in memory computation. Spark computation is based on RDDs, a data abstraction, and DAGs, representing the data manipulation processes.%

A more advanced solution is represented by Apache Spark~\cite{misc:ApacheSpark}, that allows for faster executions by avoiding or limiting the use of the persistent storage and it provides a more sophisticated programming model based on parallel execution plans (\plans) which are represented as directed acyclic graph (DAG) of phases.

%Spark uses a data abstraction called Resilient Distributed Dataset (RDD). 
%When an application is submitted to Spark, an application’s driver process is created to collect the results of the computations made on the RDDs, and the application is divided in multiple jobs. Jobs are delimited by Spark actions in the application code. Spark actions are  operations that return a value to the driver program after running a computation on an RDD.
%Spark applications consist of multiple jobs, that are delimited by Spark actions in the application code. For each job, a DAG or \textit{Parallel Execution Plan} (\plan), is created to keep track of the RDDs and maximize the parallelism while executing an application. 
%that are materialized inside the job. 
%DAG nodes represent the RDDs, meanwhile arcs represent transformations, that are the operations that create new datasets from existing ones. The application steps inside a single job are further organized into stages.
%, that are delimited by operations that require data reshuffling. The DAG defines the execution order among stages, and the extent to which stages can be executed in parallel. For each job, Spark computes a DAG or \textit{Parallel Execution Plan}, from now on called \plan to maximize the parallelism while executing an application. 
%The parallel execution plan of the application is obtained by joining the \plans of its job.
%Spark generalizes the two stage Map-Reduce to support arbitrary \plans, and provides fast data sharing between operations by executing in-memory computing, unlike Hadoop that requires intermediate data to be written to disk in order to be accessible by the following operations.
%The main advantage of Spark with respect to previous cluster computing frameworks is the fast data sharing between operations. For example, Apache Hadoop requires intermediate data to be written to disk in order to be accessible by the following operations, Spark instead allows to execute in-memory computing. 
Big data applications pose new challenges in satisfying requirements on the \textit{Quality of Service} (\qos) provided to end users. In the context of traditional applications (e.g. web) this problem has often been faced using self-adaptive systems that control runtime \textit{KPIs} (Key Performance Indicators) against changes in the application context and workload. %In the world of big data, the notion of \qos differ by application type. While interactive applications are usually assessed according to response time or throughput, 
%and their fulfillment depends on the intensity and variety of the incoming requests, 

Compared to traditional applications, where a single execution lasts ten to hundreds of milliseconds, big data applications might require minutes or hours to process large datasets, thus \qos must consider the execution of single runs. Therefore, in this domain the \qos is often defined by means of \textbf{deadline}s, that are the maximum allowed duration of as single application execution. %Thus, users may be interested in quantifying and constraining the execution time of \textit{\textbf{every single run}} of an application. 
%Cluster computing, an implementation of the Parallel Computing paradigm composed by a large number of networked multi-cpu computers in the cloud, is widely used to speed-up application execution time. 
%Clusters of networked multi-cpu computers in the cloud, implementing the Parallel Computing paradigm, are often used to speed-up the execution of big data applications. However, a fast 
%A fast computing system alone is not enough to guarantee that a big data application will meet a fixed deadline, as the execution time depends on many variable factors such as the amount of computing resources available (cpu's, memory, storage). Since the amount of work to execute the application is not known a priori and the deadline is fixed, the system must be able to allocate resources dynamically to the application at runtime to help meeting the \qos.
 
The execution time of big-data applications depends on many factors such as the amount of computing resources available (cpu's, memory, storage) and the concurrent execution of other applications on the same cluster. Therefore, in the literature one can find several approaches \cite{Verma2011, Hindman2011, Cheng2015} that focus on resource allocation or scheduling techniques in order to reduce deadline violations by using different techniques such as integer linear programming, machine learning and control theory.  %A fast computing system alone is not enough to guarantee that a big data application will meet a fixed deadline, as the execution time depends on many variable factors such as the amount of computing resources available (cpu's, memory, storage). Since the amount of work to execute the application is not known a priori and the deadline is fixed, the system must be able to allocate resources dynamically to the application at runtime to help meeting the \qos.

%The speed of execution of a big data application in fact depends on many factors that vary over time, such as the amount of resources available (i.e. number of cpu cores, amount of memory, storage space) or the amount of data to be processed. Since the amount of work required by the big data application is not known a priori and the deadline is fixed, the system must be capable of allocating the needed amount of resources dynamically at runtime (i.e. when they are required by the application) to meet the \qos. 
%

All of these approaches use the knowledge of the \plan to reason, predict or estimate the application execution time and work under the assumption that the application \plan does not change under different input datasets or application parameters. Unfortunately, this is true only if the application code does not contain any conditional branch whose outcome depends on user input values or the result of previous calculations involving input data. %If the  condition of uniqueness of the application \plan is violated, xSpark
Otherwise, different application \plans could be needed to describe all the possible execution flows generated by the combinations of all the different branch outcomes in the application code. Without considering all these \plans the analysis of the application could be significantly unprecise.


xSpark, developed at Politecnico di Milano, is an extension of the Apache Spark framework that offers fine-grained dynamic resource allocation using lightweight containers and control-theoretical planners. It allows users to set application deadlines (which is not possible using Spark) and uses the knowledge of the application \plan, retrieved during a profiling phase, to dynamically allocate resources to the application.
xSpark does not consider loops or conditional branches in the application code and assumes that the \plan is unique.

 %, composed by a centralized heuristic and a distributed local controller.
%Here is where \tool, the project described in this thesis, comes into play. 
\tool, the solution described in this thesis, extends xSpark capability to safely run multi-\plan applications, by exploiting symbolic execution. At each decisional branch outcome in the application, \tool determines which \plans are still valid and prunes the \plans tree, removing the invalid \plans, thus leaving only the valid ones in the \plans tree. A heuristic is used to select the \plan to execute among the valid ones, in order to minimize the risk of missing the deadline while maximizing the CPU usage efficiency.


%\paragraph{Results}
%The solution presented in this thesis is \tool, a toolchain providing the capability to manage the efficient execution of deadline-based QoS constrained multi-\plan Spark applications. \tool is the result of the integration of \dSymb, a tool exploiting symbolic execution techniques to generate the path condition associated to each possible {\plan}s produced by different inputs and parameters, with (a modified version of) xSpark. Moreover, \tool generates a launcher with a synthesized dataset for each \plan and \textit{GuardEvaluator}, an artifact that retrieves the feasible {\plan}s given a set of symbolic variables resolved to a value. Finally, we integrated this approach with xSpark, an extension of Spark that can control the duration of Spark applications according to specified deadlines through dynamic resource allocation. 
\tool is the result of the integration of \dSymb, a tool exploiting symbolic execution to discover all possible application {\plan}s produced by different inputs and parameters, with (a modified version of) xSpark.
%We tested \tool with two applications, Promocalls, that was developed at Politecnico di Milano in the Deib Labs\footnote{\url{https://github.com/seepep/promocalls}}, a paradigmatic application resembling a daily routine of a telecommunications company for calculating the application of promotional discounts to the most active users, and Louvain, a Spark implementation of the Louvain algorithm~\cite{Louvain} that we downloaded from a highly ranked GitHub repository\footnote{\url{https://github.com/Sotera/spark-distributed-louvain-modularity}}. Louvain uses \textit{GraphX}, a Spark library specialized for graph processing, suitable for representing large user networks and analyzing communities belonging to these networks.
We tested \tool with two applications, Promocalls and Louvain, that uses \textit{GraphX}, a Spark library specialized for graph processing.
%The evaluation shows that \approach is able to effectively extract all the \plans generated by Spark applications and that \tool effectively and efficiently controls the allocation of resources during the execution of PromoCalls and Louvain, keeping the execution times within considered deadlines with significantly smaller errors and consuming a lower amount of resources than the original version of \cSpark.
The evaluation shows that \approach is able to effectively extract all the \plans generated by Spark applications and that \tool effectively and efficiently controls the allocation of resources during the execution of PromoCalls and Louvain, keeping the execution times within considered deadlines with significantly smaller errors and consuming a lower amount of resources than the original version of \cSpark.

%\paragraph{Future Developments}
%The applicability of the profiling contained in \tool is currently limited by the  cardinality of the set of paths to be profiled, since it requires the profiling of the entire application using a launcher specific for each path identified by \dSymb. This effort can  become practically unfeasible if the number of paths is too high. A future work could be directed at improving this part of the tool chain, by moving away from the current profiling path-based selection criteria in favour of a profiling  that uses branch-based criteria.
Since the current solution focuses on controlling a single application, a future work could be directed at extending \tool to control multiple concurrent applications.
%Another path to explore with a future work is the applicability of the proposed solution to the execution of non-strict deadlines \qos-constrained multi-\plans applications.

\vfill
\newpage
\pdfbookmark[1]{Sommario}{Sommario}
\chapter*{Sommario}
\lettrine[lines=4]{\textcolor{purple}{I}}{l} bisogno di elaboraree una quantità sempre crescente di dati generati dalle moderne applicazioni sta guidando una crescente domanda di potenza di calcolo flessibile, spesso soddisfatta dalle soluzioni di cloud computing. Il cloud computing ha rivoluzionato il modo in cui le infrastrutture informatiche vengono astratte e utilizzate sfruttando la virtualizzazione e fornendo un facile accesso alle risorse attraverso Internet~\cite{articleBigData:2017}.


I big data sono un campo di ricerca che si occupa dei modi per analizzare ed estrarre informazioni da set di dati contenenti dati strutturati e non strutturati, la cui dimensione è così grande che ne rende molto difficile o praticamente impossibile l'elaborazione con database e applicazioni tradizionali. L'elaborazione di questi dati richiede quindi l'uso di framework distribuiti specializzati per l'esecuzione parallela di programmi, come Apache Hadoop ~\cite{misc:ApacheHadoop} e Apache Spark~\cite{misc:ApacheSpark}. Questi framework specializzati sono utilizzati per trasformare le applicazioni in parti atomiche che possono essere eseguite in un cluster distribuiti di macchine fisiche o virtuali.


Hadoop utilizza il modello di programmazione Map-Reduce che è stato inizialmente introdotto da Google~\cite{misc:GoogleMapReduce} e si compone di due fasi distinte: Map e Reduce.


Il primo processa e \textit{trasforma} un blocco di dati per produrre coppie chiave-valore (tuple) come output intermedi, quest'ultimo \textit{aggrega} queste tuple in un output finale.

Una soluzione più avanzata è rappresentata da Apache Spark~\cite{misc:ApacheSpark}, che consente esecuzioni più rapide evitando o limitando l'utilizzo dell'archiviazione persistente e fornisce un modello di programmazione più sofisticato basato su piani di esecuzione paralleli (\plans) che sono rappresentati come un grafico aciclico orientato (DAG) delle fasi.

Le applicazioni di big data pongono nuove sfide nel soddisfare i requisiti sulla \textit{Qualità del Servizio} (\qos) fornita agli utenti finali. Nel contesto delle applicazioni tradizionali (ad esempio le applicazioni web), questo problema è stato spesso affrontato utilizzando sistemi autoadattativi che controllano \textit {KPIs} (Key Performance Indicators) a runtime rispetto ai cambiamenti nel contesto dell'applicazione e nel carico di lavoro.

Rispetto alle applicazioni tradizionali, dove una singola esecuzione dura da decine a centinaia di millisecondi, le applicazioni di big data potrebbero richiedere minuti o ore per elaborare grandi dataset, quindi il  \qos deve considerare l'esecuzione di singole esecuzioni. Pertanto, in questo dominio il \qos viene spesso definito attraverso \textbf{deadline}, che sono la durata massima consentita della singola esecuzione  dell'applicazione.

Il tempo di esecuzione delle applicazioni Big Data dipende da molti fattori come la quantità di risorse di calcolo disponibili (CPU, memoria, storage) e l'esecuzione simultanea di altre applicazioni nello stesso cluster. Pertanto, in letteratura si possono trovare diversi approcci \cite{Verma2011, Hindman2011, Cheng2015} che si concentrano sull'assegnazione delle risorse o sulle tecniche di schedulazione al fine di ridurre le violazioni delle scadenze utilizzando diverse tecniche come la programmazione lineare intera, l'apprendimento automatico e la teoria del controllo.

Tutti questi approcci utilizzano la conoscenza del \plan per ragionare, prevedere o stimare il tempo di esecuzione dell'applicazione e lavorare partendo dal presupposto che il \plan dell'applicazione non cambi a seconda dei diversi dataset di input o parametri dell'applicazione. Sfortunatamente, questo è vero solo se il codice dell'applicazione non contiene alcun branch condizionale il cui risultato dipende dai valori di input dell'utente o dal risultato di calcoli precedenti che coinvolgono dati di input. In caso contrario, potrebbero essere necessari diversi \plans delle applicazioni per descrivere tutti i possibili flussi di esecuzione generati dalle combinazioni di tutti i diversi risultati che  derivano dal codice dell'applicazione. Senza considerare tutti questi \plans, l'analisi dell'applicazione potrebbe essere notevolmente imprecisa.

xSpark, sviluppato al Politecnico di Milano, è un'estensione del framework Apache Spark che offre un'allocazione dinamica delle risorse a grana fine utilizzando contenitori leggeri e pianificatori teorici del controllo. Consente agli utenti di impostare le deadline delle applicazioni (cosa non  possibile utilizzando Spark) e utilizza la conoscenza del \plan dell'applicazione, recuperato durante una fase di profilazione, per allocare dinamicamente le risorse all'applicazione.
xSpark non considera loop o branch condizionali nel codice dell'applicazione e presuppone che il  \plan sia univoco.

\tool, la soluzione descritta in questa tesi, estende la capacità di xSpark per eseguire in sicurezza applicazioni multi-\plan, sfruttando l'esecuzione simbolica. Ad ogni risultato di un branch decisionale nell'applicazione, \tool determina quali \plans sono ancora validi e pota l'albero dei \plans, rimuovendo i \plans non più validi, lasciando così solo quelli validi nell'albero dei \plans. Una euristica viene utilizzata per selezionare il \plan da eseguire tra quelli validi, al fine di ridurre al minimo il rischio di oltrepassare la deadline, massimizzando l'efficienza di utilizzo delle CPU.

\tool è il risultato dell'integrazione di \dSymb, uno strumento che sfrutta l'esecuzione simbolica per scoprire tutti i possibili {\plan} delle applicazioni prodotte da diversi input e parametri, con una versione modificata di xSpark.

Abbiamo testato \tool con due applicazioni, Promocalls e Louvain, che utilizza \textit{GraphX}, una libreria Spark specializzata per l'elaborazione di grafi.

La valutazione mostra che \approach è in grado di estrarre efficacemente tutti i \plans generati dalle applicazioni Spark e che \tool controlla in modo efficace ed efficiente l'allocazione delle risorse durante l'esecuzione di PromoCalls e Louvain, mantenendo i tempi di esecuzione entro scadenze prefissate con errori significativamente minori e consumando una quantità di risorse inferiore rispetto alla versione originale di \cSpark.

Poiché la soluzione attuale si concentra sul controllo di una singola applicazione, un lavoro futuro potrebbe essere diretto all'estensione di \tool per controllare più applicazioni concorrenti.

\endgroup